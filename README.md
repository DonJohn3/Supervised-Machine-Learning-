# Supervised-Machine-Learning-
The data utilized for modeling has an extensive set of personal information about each applicant. The dataset comes as two separate datasets, an “application record” and a “credit record.” In the “application record” dataset, there are a total of 17 variables, 12 categorical and 5 continuous. In the “credit record” dataset, there are 2 variables, 1 categorical and 1 continuous, both datasets contain an ID key column to tie a candidate's application record with their credit data. The dataset prior to any processing comprises of 537,667.00 observations. The objective is to work with this dataset to determine principal factors & build classification models to predict whether a candidate should receive a credit card. The team utilized four supervised machine learning models: Gaussian Naïve Bayes, Logistic Regression, AdaBoost, and Random Forest. One of the challenges the team encountered when building models for this project was the imbalance between customers who were classified as risky and not risky. This imbalance made it difficult for any of the models to distinguish the differences between the two classes. To cure the imbalance issue in the training set, the team used an advanced oversampling technique called SMOTE. Out of the total 8 models that were built, base models plus the hyper-tuned models, the best model are the random forest models. This model produced the least number of false negatives, false positives, a high accuracy score, and the best F1 score out of the set. In this paper, the team will discuss the data preparation, the methods used to cure imbalance in data, the math behind the algorithms, results of the models, and discuss how the research could be improved.
